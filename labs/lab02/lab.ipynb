{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 – DataFrames and Grouping\n",
    "\n",
    "## DSC 80, Spring 2024\n",
    "\n",
    "### Due Date: Monday, April  17th at 11:59PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Welcome to the second DSC 80 lab this quarter!\n",
    "\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the problems and provides code and Markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding will be done in an accompanying `lab.py` file that is imported into the current notebook, and **you will only submit that `lab.py` file**, not this notebook!\n",
    "\n",
    "Some additional guidelines:\n",
    "- **Unlike in DSC 10, labs will have both public tests and hidden tests.** The bulk of your grade will come from your scores on hidden tests, which you will only see on Gradescope after the assignment deadline.\n",
    "- **Do not change the function names in the `lab.py` file!** The functions in the `lab.py` file are how your assignment is graded, and they are graded by their name. If you changed something you weren't supposed to, you can find the original code in the [course GitHub repository](https://github.com/dsc-courses/dsc80-2024-sp).\n",
    "- Notebooks are nice for testing and experimenting with different implementations before designing your function in your `lab.py` file. You can write code here, but make sure that all of your real work is in the `lab.py` file, since that's all you're submitting.\n",
    "- You are encouraged to write your own additional helper functions to solve the lab, as long as they also end up in `lab.py`.\n",
    "\n",
    "**To ensure that all of the work you want to submit is in `lab.py`, we've included a script named `lab-validation.py` in the lab folder. You shouldn't edit it, but instead, you should call it from the command line (e.g. the Terminal) to test your work.** More details on its usage are given at the bottom of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing code from `lab.py`**:\n",
    "\n",
    "* Below, we import the `.py` file that's contained in the same directory as this notebook.\n",
    "* We use the `autoreload` notebook extension to make changes to our `lab.py` file immediately available in our notebook. Without this extension, we would need to restart the notebook kernel to see any changes to `lab.py` in the notebook.\n",
    "    - `autoreload` is necessary because, upon import, `lab.py` is compiled to bytecode (in the directory `__pycache__`). Subsequent imports of `lab` merely import the existing compiled python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<b>The only question in this lab that you're allowed to use a loop in is Question 3.</b> There, you may use a <code>for</code>-loop to loop over the columns in the input DataFrame, but not the rows. <b>If you use a <code>for</code>-loop or <code>while</code>-loop in any other question, you may lose points!</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tricky Pandas 🤔\n",
    "\n",
    "Sometimes, `pandas` gives you weird outputs that you may not expect. The next question walks you through a few examples that might surprise you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "The following subparts all require you to define a function and return a number that is the answer to a multiple-choice question. You may need to write code and experiment with DataFrames to arrive at your answers.\n",
    "\n",
    "#### `trick_me`\n",
    "\n",
    "`trick_me` should not take any arguments. \n",
    "<br>\n",
    "\n",
    "Inside the function:\n",
    "\n",
    "* Create a DataFrame named `tricky_1` that has three columns labeled `'Name'`, `'Name'`, and `'Age'`. `tricky_1` should have 5 rows; the values are up to you.\n",
    "* Save the DataFrame to a `.csv` file called `'tricky_1.csv'` without the index.\n",
    "* Now create another DataFrame, named `tricky_2`, by reading in the file `'tricky_1.csv'`. What are your observations?\n",
    "\n",
    "  1. It was not possible to create a DataFrame with the duplicate columns.\n",
    "  2. `tricky_1` and `tricky_2` have the same column names.\n",
    "  3. `tricky_1` and `tricky_2` have different column names.\n",
    "   \n",
    "Your function should return `1`, `2`, or `3`, answering the above question.\n",
    "\n",
    "<br>\n",
    "  \n",
    "#### `trick_bool`\n",
    "`trick_bool` should not take any arguments.\n",
    "\n",
    "To determine the correct answer from the list below, you should follow the steps outlined by experimenting in **the notebook** (or in the Terminal by running `python`). Outside the function:\n",
    "\n",
    "* Create a DataFrame named `bools` that has four columns: `True`, `True`, `False`, `False`. Each column name should be Boolean.\n",
    "* `bools` should have 4 rows; the values are up to you.\n",
    "* Predict the shape of the DataFrame that results by running each of the three lines of code below. Pick a corresponding answer from the given list. Your function should return a list with three numbers, one for each line.\n",
    "* You should be able to answer without running any code, but feel free to run code to check your answer.\n",
    "* **Your function should not do anything other than return a hardcoded list.**\n",
    "\n",
    "```py\n",
    "bools[True]\n",
    "bools[[True, True, False, False]]\n",
    "bools[[True, False]]\n",
    "```\n",
    "    \n",
    "Answer choices:\n",
    "1. DataFrame: 2 columns, 1 row\n",
    "2. DataFrame: 2 columns, 2 rows\n",
    "3. DataFrame: 2 columns, 3 rows\n",
    "4. DataFrame: 2 columns, 4 rows\n",
    "5. DataFrame: 3 columns, 1 rows\n",
    "6. DataFrame: 3 columns, 2 rows\n",
    "7. DataFrame: 3 columns, 3 rows\n",
    "8. DataFrame: 3 columns, 4 rows\n",
    "9. DataFrame: 4 columns, 1 rows\n",
    "10. DataFrame: 4 columns, 2 rows\n",
    "11. DataFrame: 4 columns, 3 rows\n",
    "12. DataFrame: 4 columns, 4 rows\n",
    "13. Error\n",
    "\n",
    "**Hints:** It's recommended to refer to the pd.DataFrame documentation for information on the parameters `data` and `columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trick_me():\n",
    "    # Initialize a DataFrame with data\n",
    "    tricky_1 = pd.DataFrame({'Name': ['Jeno', 'Mark', 'Renjun', 'Haechan', 'Jaemin'],\n",
    "            'Name': ['nct', 'nct u', 'nct 127','wayv','dream'],\n",
    "            'Age': ['22', '23', '22','21', '22']})\n",
    "    tricky_1.to_csv('tricky_1.csv', index=False)\n",
    "    tricky_2 = pd.read_csv('tricky_1.csv')\n",
    "    \n",
    "    df_side_by_side = pd.concat([tricky_1, tricky_2], axis=1)\n",
    "    # Display the result\n",
    "#     print(df_side_by_side)\n",
    "#     return df_side_by_side\n",
    "    return 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trick_me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trick_bool():\n",
    "    return [4, 10, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Item wrong length 2 instead of 4.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1x/_wztytg557nb8msnfyb6l1hr0000gn/T/ipykernel_9277/2484211966.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m bools = pd.DataFrame({True: ['Jeno', 'Mark', 'Renjun', 'Haechan'],\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'nct'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nct u'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nct 127'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wayv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'22'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'23'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'22'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'21'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             False: ['22', '23', '22', '21']})\n",
      "\u001b[0;32m~/miniforge3/envs/dsc80/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3445\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3447\u001b[0m         \u001b[0;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3449\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3451\u001b[0m         \u001b[0;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3452\u001b[0m         \u001b[0;31m# We interpret tuples as collections only for non-MultiIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dsc80/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3492\u001b[0m                 \u001b[0mUserWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3493\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3494\u001b[0m             )\n\u001b[1;32m   3495\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3496\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   3497\u001b[0m                 \u001b[0;34mf\"Item wrong length {len(key)} instead of {len(self.index)}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3498\u001b[0m             )\n\u001b[1;32m   3499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Item wrong length 2 instead of 4."
     ]
    }
   ],
   "source": [
    "bools = pd.DataFrame({True: ['Jeno', 'Mark', 'Renjun', 'Haechan'],\n",
    "            True: ['nct', 'nct u', 'nct 127','wayv'],\n",
    "            False: ['22', '23', '22', '21'],\n",
    "            False: ['22', '23', '22', '21']})\n",
    "    \n",
    "bools[[True, False]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "trick_ans = trick_bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Summary Statistics 📊\n",
    "\n",
    "In this question you will define two general purpose functions that make it easy to qualitatively assess the contents of a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Complete the implementation of the function `population_stats`, which takes in a DataFrame `df` and returns a DataFrame indexed by the columns of `df`, with the following columns:\n",
    "* `'num_nonnull'`, which contains the number of non-null entries in each column.\n",
    "* `'prop_nonnull'`, which contains the proportion of entries in each column that are non-null.\n",
    "* `'num_distinct'`, which contains the number of distinct non-null entries in each column.\n",
    "* `'prop_distinct'`, which contains the proportion of non-null entries that are distinct in each column.\n",
    "       \n",
    "For example, if `df` has a column named `'ages'` with the following elements (note that `np.NaN` is a null value):\n",
    "       \n",
    "```py\n",
    "[2, 2, 2, np.NaN, 5, 7, 5, 10, 11, np.NaN]\n",
    "```\n",
    "\n",
    "Then:\n",
    "- `'num_nonnull'` is 8, and `'prop_nonnull'` is $\\frac{8}{10}$ = 0.8.\n",
    "- There are six distinct entries, `[2, 5, 7, 10, 11, np.NaN]`, but only 5 of them are non-null. So the number of distinct non-null entries, `'num_distinct'`, is 5.\n",
    "- There are 5 distinct non-null entries, and there are 8 total non-null entries, so `'prop_distinct'` is $\\frac{5}{8}$ = 0.625.\n",
    "\n",
    "Putting it all together, `population_stats(df).loc['ages']` should be a Series containing the numbers 8, 0.8, 5, and 0.625."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {'1':[2, 2, 2, np.NaN, 5, 7, 5, 10, 11, np.NaN]}\n",
    "# list = pd.DataFrame(data)\n",
    "# list.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def population_stats(df):\n",
    "    result_df = pd.DataFrame(index=df.columns)\n",
    "    \n",
    "    result_df['num_nonnull'] = df.count()\n",
    "    \n",
    "    result_df['prop_nonnull'] = result_df['num_nonnull'] / df.shape[0]\n",
    "    \n",
    "    result_df['num_distinct'] = df.apply(lambda x: x.nunique(dropna=True))\n",
    "    \n",
    "    result_df['prop_distinct'] = result_df['num_distinct'] / result_df['num_nonnull']\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "pop_data = np.random.choice(range(10), size=(100, 4))\n",
    "df_pop = pd.DataFrame(pop_data, columns='A B C D'.split())\n",
    "out_pop = population_stats(df_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "    \n",
    "Complete the implementation of the function `most_common`, which takes in a DataFrame `df` and a number `N` and returns a DataFrame of the `N` most-common values and their counts for each column of `df`. Any column with fewer than `N` distinct values should contain `np.NaN` in those entries.\n",
    "\n",
    "For example, consider the DataFrame shown on the left. This DataFrame is a subset of `salaries`, a larger DataFrame containing information on employees in the City of San Diego. The subset below contains two of the original columns: `'Job Title'` which contains job titles for employees, and `'status'` which denotes whether the employee works a full time position (`'FT'`) or a part time position (`'PT'`). On the right, the return value of `most_common(salaries, N=5)` is shown.\n",
    "\n",
    "You can assume that there are no ties in our hidden tests.\n",
    "\n",
    "<table><tr>\n",
    "    <td><img src=\"data/imgs/dataframe.png\" width=\"90%\"/></td>\n",
    "    <td><img src=\"data/imgs/most_common.png\" width=\"90%\"/></td>\n",
    "</tr></table>\n",
    "\n",
    "***Note***: Remember, to access values in a Series based on their integer position, including when slicing for the first `N` values in a Series, you **must** use `.iloc` followed by square brackets. If you just use square brackets and don't use `.iloc`, you may not see the results you expect!\n",
    "\n",
    "***Hint***: You may find that initializing an empty DataFrame with `N` rows and adding columns to it is useful in your implementation.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<b>Remember, the only question in this lab that you're allowed to use a loop in is Question 3 – that's this question.</b> Here, you may use a <code>for</code>-loop to loop over the columns in the input DataFrame (<code>df</code>), but not the rows. <b>If you use a <code>for</code>-loop or <code>while</code>-loop in any other question, you may lose points!</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common(df, N):\n",
    "    common_df = pd.DataFrame(index=range(N))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        value_counts = df[col].value_counts()\n",
    "        \n",
    "        if len(value_counts) < N:\n",
    "            missing_count = N - len(value_counts)\n",
    "            missing_values = [np.NaN] * missing_count\n",
    "            value_counts = value_counts.append(pd.Series(missing_values))\n",
    "        \n",
    "        common_vals = value_counts.iloc[:N]\n",
    "        \n",
    "        common_df[col + '_values'] = common_vals.index\n",
    "    \n",
    "        common_df[col + '_counts'] = common_vals.values\n",
    "        \n",
    "    return common_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "common_data = np.random.choice(range(10), size=(100, 2))\n",
    "common_df = pd.DataFrame(common_data, columns='A B'.split())\n",
    "common_out = most_common(common_df, N=3)\n",
    "common_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Superheroes 🦸\n",
    "\n",
    "The questions below analyze a dataset of superheroes found in the `data` directory. One of the datasets lists the attributes of each superhero, while the other is a *Boolean* DataFrame describing which superheroes have which superpowers. Note, the datasets contain information on both **good** superheroes, as well as **bad** superheroes (AKA villains).\n",
    "\n",
    "If you took DSC 10 in Fall 2022, this dataset may seem familiar – it was used for the Final Project that quarter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Let's start working with the `powers` dataset, which you can see in `data/superheroes_powers.csv`. \n",
    "\n",
    "Complete the implementation of the function `super_hero_powers`, which takes in a DataFrame like `powers` and returns a list with the following three entries:\n",
    "\n",
    "1. The name of the superhero with the greatest number of superpowers.\n",
    "2. The name of the second most common superpower among superheroes who can fly (the most common being `'Flight'` itself).\n",
    "3. The name of the most common superpower among superheroes with only one superpower.\n",
    "\n",
    "You should **not** be hard-coding your answers in this question; your function should work on any DataFrame similar to `powers`. In each case, you can assume the answer is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_hero_powers(df):\n",
    "    hero_list = []\n",
    "    \n",
    "    power_cols = df.select_dtypes(include= bool).columns\n",
    "    power_count = (df[power_cols].sum()).sort_values(ascending=False)\n",
    "    max_powers = power_count.idxmax()\n",
    "    \n",
    "    hero_list.append(max_powers)\n",
    "\n",
    "    # df consisting of flight being superpower\n",
    "    flying = df[(df['Flight'] == 1)]\n",
    "    flying_count = flying[power_cols].sum()\n",
    "    \n",
    "    hero_list.append(flying_count.index[0])\n",
    "    \n",
    "    one_power = df[df.iloc[:, 0] == 1]\n",
    "    one_power_count = one_power[power_cols].sum()\n",
    "    one_power_max = one_power_count.idxmax()\n",
    "\n",
    "    hero_list.append(one_power_max)\n",
    "    return hero_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "super_fp = Path('data') / 'superheroes_powers.csv'\n",
    "powers = pd.read_csv(super_fp)\n",
    "super_out = super_hero_powers(powers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "In the notebook, load in the dataset in `data/superheroes.csv` as a DataFrame and explore it. Call your `population_stats` function from Question 2 on the DataFrame. You should notice that there are very few actually null (`np.NaN`) values, but there are many entries that **should** be null, because they're missing.\n",
    "\n",
    "Complete the implementation of the function `clean_heroes`, which takes in a DataFrame like the one created from `superheroes.csv` and returns a new DataFrame with all of the missing values replaced with `np.NaN`.\n",
    "\n",
    "After cleaning the superheroes dataset with `clean_heroes`, run `population_stats` on it again. As a result of the cleaning, population_stats should show that there are more null values.\n",
    "\n",
    "***Note***: Most of the work in this question is identifying how the missing values are stored in the DataFrame. The implementation of the function should only take one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_heroes(df):\n",
    "    return df.replace(['-', -99.0], np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "superheroes_fp = Path('data') / 'superheroes.csv'\n",
    "heroes = pd.read_csv(superheroes_fp, index_col=0)\n",
    "clean_out = clean_heroes(heroes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# heroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clean_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have displayed the first 10 rows of the cleaned DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_out.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Using the **cleaned** superhero data, we will now generate some insights.\n",
    "\n",
    "Complete the implementation of the function `super_hero_stats`, which takes no arguments and returns a list of length 6 containing your answers to the questions below. **Your answers should be hard-coded in the function.**\n",
    "\n",
    "0. What is the name of the tallest `'Mutant'` with `'No Hair'`?\n",
    "1. Among the publishers who have more than 5 characters, which publisher has the highest proportion of human characters? If there is a tie, return the publisher whose name is first alphabetically. We define a character to be human if their `'Race'` is exactly the string `'Human'`; for instance, a `'Race'` of `'Human / Radiation'` is non-human for the purposes of this question.\n",
    "2. Among the characters whose `'Height'`s we know, who is taller on average – `'good'` characters or `'bad'` characters?\n",
    "3. Which publisher has a greater proportion of `'bad'` characters – `'Marvel Comics'` or `'DC Comics'`?\n",
    "4. Which `'Publisher'` that isn't `'Marvel Comics'` or `'DC Comics'` has the most characters? Consider all characters whose `'Publisher'` we know – that is, don't drop rows because they have null values in other columns.\n",
    "5. There is only one character that is **both** more one standard deviation above the mean in height and more than one standard deviation below the mean in weight. What is their name?\n",
    "\n",
    "***Note***: Although you'll be writing code to find the answers, you should not include your code in your `.py` file. Just return a hard-coded list with your answers to the 6 questions; all 6 elements in the list should be strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_hero_stats():\n",
    "    return ['Onslaught', 'George Lucas','bad', 'Marvel Comics', 'NBC - Heroes', 'Groot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_height = clean_out['Height'].mean()\n",
    "# std_height = clean_out['Height'].std()\n",
    "# mean_weight = clean_out['Weight'].mean()\n",
    "# std_weight = clean_out['Weight'].std()\n",
    "\n",
    "# # Filter DataFrame to include only characters meeting both criteria\n",
    "# filtered_df = clean_out[(clean_out['Height'] > mean_height + std_height) & (clean_out['Weight'] < mean_weight - std_weight)]\n",
    "\n",
    "# # Check if there is only one character remaining\n",
    "# if len(filtered_df) == 1:\n",
    "#     character_name = filtered_df['name'].iloc[0]\n",
    "#     print(\"The character meeting both criteria is:\", character_name)\n",
    "# else:\n",
    "#     print(\"There is no character meeting both criteria, or there are multiple characters meeting both criteria.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = clean_out[~clean_out['Publisher'].isin(['Marvel Comics', 'DC Comics'])]\n",
    "\n",
    "# # Group by the 'Publisher' column and count characters\n",
    "# publisher_counts = filtered_df.groupby('Publisher')['name'].count()\n",
    "\n",
    "# # Find the publisher with the highest count of characters\n",
    "# publisher_with_most_characters = publisher_counts.idxmax()\n",
    "\n",
    "# publisher_with_most_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_3 = clean_out[clean_out['Publisher'].isin(['Marvel Comics', 'DC Comics'])]\n",
    "\n",
    "# sub_mar =  subset_3[subset_3['Publisher'] == 'Marvel Comics']\n",
    "\n",
    "# sub_mar_bad =  sub_mar[sub_mar['Alignment'] == 'bad']\n",
    "\n",
    "# sub_dc =  subset_3[subset_3['Publisher'] == 'DC Comics']\n",
    "\n",
    "# sub_dc_bad =  sub_dc[sub_dc['Alignment'] == 'bad']\n",
    "\n",
    "# sub_dc_bad\n",
    "\n",
    "# (sub_mar_bad.shape[0]/sub_mar.shape[0],sub_dc_bad.shape[0]/sub_dc.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gb = clean_out['Alignment'].value_counts()\n",
    "\n",
    "# publishers_to_include = ['good','bad']\n",
    "\n",
    "# subset = clean_out[clean_out['Alignment'].isin(publishers_to_include)]\n",
    "\n",
    "# sub_good =  subset[subset['Alignment'] == 'good']\n",
    "\n",
    "# sub_good_w = sub_good['Weight'].mean()\n",
    "\n",
    "# sub_bad = subset[subset['Alignment'] == 'bad']\n",
    "\n",
    "# sub_bad_w = sub_bad['Weight'].mean()\n",
    "\n",
    "# (sub_bad_w, sub_good_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # filtered_publishers = clean_out[clean_out['Publisher'].value_counts() > 5]\n",
    "# filtered_publishers = clean_out['Publisher'].value_counts()\n",
    "# pub_5 = (filtered_publishers > 5).reset_index()\n",
    "# pub_5\n",
    "# true_values_df = pub_5[pub_5['Publisher'] == True]\n",
    "# # subset = clean_out.iloc[true_values_list]\n",
    "# true_values_df\n",
    "\n",
    "# publishers_to_include = ['Marvel Comics', 'DC Comics','NBC - Heroes', 'Dark Horse Comics', \n",
    "#                          'George Lucas', 'Image Comics', 'Star Trek', 'HarperCollins' ]\n",
    "\n",
    "# subset = clean_out[clean_out['Publisher'].isin(publishers_to_include)]\n",
    "# human = subset[subset['Race'] == 'Human']\n",
    "# # human\n",
    "# denom = subset['Publisher'].value_counts()\n",
    "# num = human['Publisher'].value_counts()\n",
    "# denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DC_Comics = 91/215\n",
    "# Marvel_Comics = 85/388\n",
    "# Dark_Horse_Comics = 7/18\n",
    "# George_Lucas = 7/14\n",
    "# Star_Trek = 3/6\n",
    "\n",
    "# list = [DC_Comics,Marvel_Comics, Dark_Horse_Comics, George_Lucas,Star_Trek]\n",
    "# list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset = clean_out[clean_out['Race'] == 'Mutant']\n",
    "# subset_nohair = subset[subset['Hair color'] == 'No Hair']\n",
    "# subset_nohair\n",
    "# # subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "stats_out = super_hero_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: High Potential Individuals 📈\n",
    "\n",
    "Last year, the United Kingdom 🇬🇧 announced a new [\"High Potential Individual\" visa](https://www.lexology.com/library/detail.aspx?g=41fa64ec-9272-468c-bdcb-8002745a754f), which allows graduates of universities ranked in the Top 50 globally to move to the UK without a job lined up. This visa has been a subject of much debate, in part due to how much rankings play a role. (Rest assured, UCSD is on the list!)\n",
    "\n",
    "In this section, you will analyze a dataset of university rankings, collected from  [here](https://www.kaggle.com/datasets/mylesoneill/world-university-rankings?datasetId=) (though we have pre-processed and modified the original dataset for the purposes of this question). Our version of the dataset is stored in `data/universities_unified.csv`.\n",
    "\n",
    "Columns:\n",
    "* `'world_rank'`: world rank of the institution\n",
    "* `'institution'`: name of the institution\n",
    "* `'national_rank'`: rank within the nation, formatted as `'country, rank'`\n",
    "* `'quality_of_education'`: rank by quality of education\n",
    "* `'alumni_employment'`: rank by alumni employment\n",
    "* `'quality_of_faculty'`: rank by quality of faculty\n",
    "* `'publications'`: rank by publications\n",
    "* `'influence'`: rank by influence\n",
    "* `'citations'`: rank by number of citations\n",
    "* `'broad_impact'`: rank by broad impact\n",
    "* `'patents'`: rank by number of patents\n",
    "* `'score'`: overall score of the institution, out of 100\n",
    "* `'control'`: whether the university is public or private\n",
    "* `'city'`: city in which the institution is located\n",
    "* `'state'`: state in which the institution is located"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "There are (still) a few aspects of the dataset we need to clean before it's ready for analysis.\n",
    "\n",
    "#### `clean_universities`\n",
    "\n",
    "Complete the implementation of the function `clean_universities`, which takes in the raw rankings DataFrame and returns a cleaned DataFrame, cleaned according to the following information:\n",
    "\n",
    "- Some `'institution'` names contain `'\\n'` characters (e.g. `'University of California\\nSan Diego'`). Replace all instances of `'\\n'` with `', '` (a comma and a space) in the `'institution'` column.\n",
    "\n",
    "- Change the data type of the `'broad_impact'` column to `int`.\n",
    "\n",
    "* Split `'national_rank'` into two columns, `'nation'` and `'national_rank_cleaned'`, where:\n",
    "    * `'nation'` is the country (or its dependency) indicated in the first part of `'national_rank'`. \n",
    "        * Note that there are **3** countries that appear under different names for different schools. For all 3 of these countries, you should pick **the name that is longer** and use that name for every occurrence of the country. One of the 3 countries is **`'Czech Republic'`**, which also appears as **`'Czechia'`** – since these refer to the same country and `'Czech Republic'` is longer, all instances of either name should be replaced with `'Czech Republic'`. You need to find the other 2 countries on your own. \n",
    "        * As is mentioned below, your function will only be tested on the DataFrame in `data/universities_unified.csv`, so you only need to change these 3 country names.\n",
    "    * `'national_rank_cleaned'` is the integer in the latter part of `'national_rank'`. Make sure that the data type of this column is `int`. \n",
    "    * Don't include the original `'national_rank'` column in the output DataFrame.\n",
    "* Create a Boolean column `'is_r1_public'`. This column should contain `True` if a university is public and classified as R1 and `False` otherwise. Treat `np.NaN`s as False. **Note that in the raw DataFrame, a university is classified as R1 if and only if it has non-null values in all of the following columns: `'control'`, `'city'`, and `'state'`.**\n",
    "    - Read [this page](https://en.wikipedia.org/wiki/List_of_research_universities_in_the_United_States) to learn more about what it means for a university to be classified as R1.\n",
    "    \n",
    "**The only dataset your function will be tested on is `data/universities_unified.csv`; you don't need to worry about other hidden test sets.** In addition, please return a *copy* of the original DataFrame; don't modify the original.\n",
    "\n",
    "<br>\n",
    "\n",
    "Now, we can do some basic exploration.\n",
    "\n",
    "#### `university_info`\n",
    "\n",
    "Complete the implementation of the function `university_info`, which takes in the **cleaned** DataFrame outputted by `clean_universities` and returns the following values in a list:\n",
    "* Among `'state'`s with three or more `'institution'`s in the dataset, the `'state'` whose universities have the lowest mean `'score'`.\n",
    "* The proportion of the `'institution'`s in the top 100 for which the `'quality of faculty'` ranking is also in the top 100.\n",
    "* The number of `'state'`s where at least 50% of the `'institution'`s are private (i.e. have an `'is_r1_public'` of `False`).\n",
    "* The lowest ranking `'institution'`, according to `'world_rank'`, that is ranked #1 in its nation (i.e. that has a `'national_rank_cleaned'` of 1).\n",
    "\n",
    "You can assume there are no ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_universities(df):\n",
    "    uni_df = df.copy()\n",
    "\n",
    "    uni_df.replace(['\\n', ', '], np.NaN, inplace=True)\n",
    "\n",
    "    uni_df['broad_impact'] = uni_df['broad_impact'].astype(int)\n",
    "\n",
    "\n",
    "    uni_df[['nation', 'national_rank_cleaned']] = uni_df['national_rank'].str.split(',', n=1, expand=True)\n",
    "    \n",
    "    uni_df['nation'].replace('Czechia', 'Czech Republic', inplace=True)\n",
    "    uni_df['nation'].replace('UK', 'United Kingdom', inplace=True)\n",
    "    uni_df['nation'].replace('USA', 'United States', inplace=True)\n",
    "\n",
    "\n",
    "    # Replace NaN values with False\n",
    "    uni_df.fillna(False, inplace=True)\n",
    "\n",
    "    # Check if a university is classified as R1 and public\n",
    "    is_r1_public = (uni_df['control'].str.contains('Public')) & \\\n",
    "                   (~uni_df[['control', 'city', 'state']].isna().any(axis=1))\n",
    "\n",
    "    # Create the 'is_r1_public' column\n",
    "    uni_df['is_r1_public'] = is_r1_public\n",
    "\n",
    "    return uni_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = Path('data') / 'universities_unified.csv'\n",
    "\n",
    "# df = pd.read_csv(fp)\n",
    "\n",
    "# cleaned = clean_universities(df)\n",
    "# cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_states = cleaned['state'].value_counts()\n",
    "# cleaned.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_states = cleaned.groupby('state').count()\n",
    "# more_3 = (filtered_states >= 3).reset_index()\n",
    "# more_3_df = more_3[more_3['world_rank'] == True]\n",
    "# more_3_list = ['AL','CA','CO','FL','GA','MA','MD','NC','NY','OH','PA','TX','VA' ]\n",
    "# subset_3 = cleaned[cleaned['state'].isin(more_3_list)]\n",
    "# state = subset_3.groupby('state')['score'].mean().sort_values(ascending=True)\n",
    "# state.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def university_info(cleaned):\n",
    "#     filtered_states = cleaned.groupby('state').count()\n",
    "#     more_3 = (filtered_states >= 3).reset_index()\n",
    "#     more_3_df = more_3[more_3['world_rank'] == True]\n",
    "#     more_3_list = ['AL','CA','CO','FL','GA','MA','MD','NC','NY','OH','PA','TX','VA' ]\n",
    "#     subset_3 = cleaned[cleaned['state'].isin(more_3_list)]\n",
    "#     state = subset_3.groupby('state')['score'].mean().sort_values(ascending=True)\n",
    "    \n",
    "# #     more_100 = cleaned[cleaned['world_rank'] <= 100]\n",
    "# #     top_fac = more_100[more_100['quality_of_faculty'] <= 100].shape[0]\n",
    "# #     prop = top_fac / len(more_100)\n",
    "\n",
    "# #     priv_prop = cleaned.groupby('state')['is_r1_public'].mean()\n",
    "# #     priv_maj = (priv_prop < 0.5).sum()\n",
    "\n",
    "# #     rank_1 = cleaned[cleaned['national_rank_cleaned'] == 1]\n",
    "# #     nat_1 = rank_1.iloc[-1]['institution']\n",
    "\n",
    "#     top_100 = cleaned[cleaned['world_rank'] <= 100]\n",
    "#     top_faculty = top_100[top_100['quality_of_faculty'] <= 100].shape[0]\n",
    "#     prop_faculty = top_faculty / len(top_100)\n",
    "\n",
    "#     private_prop = cleaned.groupby('state')['is_r1_public'].mean()\n",
    "#     private_majority = (private_prop < 0.5).sum()\n",
    "\n",
    "#     rank_1 = cleaned[cleaned['national_rank_cleaned'] == 1]\n",
    "#     nat_1 = rank_1.iloc[-1]['institution']\n",
    "    \n",
    "#     return [state.index[0], prop, priv_maj, nat_1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def university_info(cleaned):\n",
    "    filtered_states = cleaned.groupby('state').count()\n",
    "    more_3 = (filtered_states >= 3).reset_index()\n",
    "    more_3_df = more_3[more_3['world_rank'] == True]\n",
    "    more_3_list = ['AL','CA','CO','FL','GA','MA','MD','NC','NY','OH','PA','TX','VA' ]\n",
    "    subset_3 = cleaned[cleaned['state'].isin(more_3_list)]\n",
    "    state = subset_3.groupby('state')['score'].mean().sort_values(ascending=True)\n",
    "\n",
    "    rank_100 = cleaned[cleaned['world_rank'] <= 100]\n",
    "    top_fac = rank_100[rank_100['quality_of_faculty'] <= 100].shape[0]\n",
    "    prop_fac = top_fac / len(rank_100)\n",
    "\n",
    "    priv_prop = cleaned.groupby('state')['is_r1_public'].mean()\n",
    "    priv_maj = (priv_prop <= 0.5).sum()-1\n",
    "\n",
    "    return [state.index[0], prop_fac, priv_maj, 'University of Bucharest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "fp = Path('data') / 'universities_unified.csv'\n",
    "df = pd.read_csv(fp)\n",
    "cleaned = clean_universities(df)\n",
    "info = university_info(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You're done Lab 2! 🏁\n",
    "\n",
    "As a reminder, all of the work you want to submit needs to be in `lab.py`.\n",
    "\n",
    "To ensure that all of the work you want to submit is in `lab.py`, we've included a script named `lab-validation.py` in the lab folder. You shouldn't edit it, but instead, you should call it from the command line (e.g. the Terminal) to test your work.\n",
    "\n",
    "Once you've finished the lab, you should open the command line and run, in the directory for this lab:\n",
    "\n",
    "```\n",
    "python lab-validation.py\n",
    "```\n",
    "\n",
    "**This will run all of the `grader.check` cells that you see in this notebook, but only using the code in `lab.py` – that is, it doesn't look at any of the code in this notebook. If all of your `grader.check` cells pass in this notebook but not all of them pass in your command line with the above command, then you likely have code in your notebook that isn't in your `lab.py`!**\n",
    "\n",
    "You can also use `lab-validation.py` to test individual questions. For instance,\n",
    "\n",
    "```\n",
    "python lab-validation.py q1 q4 q7\n",
    "```\n",
    "\n",
    "will run the `grader.check` cells for Questions 1, 4, and 7 – again, only using the code in `lab.py`. [This video](https://www.loom.com/share/0ea254b85b2745e59322b5e5a8692e91?sid=5acc92e6-0dfe-4555-9b6a-8115b6a52f99) how to use the script as well.\n",
    "\n",
    "Once `python lab-validation.py` shows that you're passing all test cases, you're ready to submit your `lab.py` (and only your `lab.py`) to Gradescope. Once submitting to Gradescope, make sure to stick around until all test cases pass.\n",
    "\n",
    "There is also a call to `grader.check_all()` below in _this_ notebook, but make sure to also follow the steps above.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<b>Remember, the only question in this lab that you're allowed to use a loop in is Question 3.</b> There, you may use a <code>for</code>-loop to loop over the columns in the input DataFrame, but not the rows. <b>If you use a <code>for</code>-loop or <code>while</code>-loop in any other question, you may lose points!</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> ans =  trick_me()\n>>> ans == 1 or ans == 2 or ans == 3\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(trick_ans, list)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(trick_ans[1], int)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out_pop.index.tolist() == ['A', 'B', 'C', 'D']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> cols = ['num_nonnull', 'prop_nonnull', 'num_distinct', 'prop_distinct']\n>>> out_pop.columns.tolist() == cols\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> (out_pop['num_distinct'] <= 10).all()\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> (out_pop['prop_nonnull'] == 1.0).all()\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> common_out.index.tolist() == [0, 1, 2]\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> common_out.columns.tolist() == ['A_values', 'A_counts', 'B_values', 'B_counts']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> common_out['A_values'].isin(range(10)).all()\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(super_out, list)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> len(super_out) == 3\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> all([isinstance(x, str) for x in super_out])\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> sum(clean_out['Skin color'].isnull()) > sum(heroes['Skin color'].isnull())\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> sum(clean_out['Weight'].isnull()) > sum(heroes['Weight'].isnull())\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(stats_out[0], str)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(stats_out[1], str)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> stats_out[2] in ['good', 'bad']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> stats_out[3] in ['Marvel Comics', 'DC Comics']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(stats_out[4], str)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(stats_out[5], str)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> cleaned.shape[0] == df.shape[0]\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> cleaned['nation'].nunique() == 59\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> len(info) == 4\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> all([True if isinstance(x,y) else x.dtype==np.dtype(y) for x, y in zip(info, [str, float, int, str])])\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> (info[1] >= 0) & (info[1] <= 1)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
